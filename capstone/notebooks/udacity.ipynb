{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7oagxHHofxb"
   },
   "source": [
    "# Import dataset in to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3n8bRW6WcKo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7oagxHHofxb"
   },
   "source": [
    "# Analyze train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "h0aRi6Toaq8j",
    "outputId": "5d5c6246-c8c3-49e0-e8b7-58850a83f62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample insincere questions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321903</th>\n",
       "      <td>3f161f8b7be57098b0b6</td>\n",
       "      <td>Why can women never decide what or where they want to eat?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882246</th>\n",
       "      <td>acd78680501fc908040f</td>\n",
       "      <td>Why don't the Turks join the Arabs? How can they turn away from their only brothers?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326259</th>\n",
       "      <td>3ff43a2ee7be50e25114</td>\n",
       "      <td>I am so fucking ugly even you wouldn't date me and I know it? Yo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33314</th>\n",
       "      <td>06860ff88c0d7c76fd0a</td>\n",
       "      <td>What do pro-choice people mean when they talk about the woman's body?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73556</th>\n",
       "      <td>0e69b32e3da3314aae53</td>\n",
       "      <td>What would happen if a city or an entire country made humans illegal?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396719</th>\n",
       "      <td>4db993a4950d1c3476ff</td>\n",
       "      <td>How do you stop the misinformation spread by pseudo Hindu intellectuals on Quora?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945843</th>\n",
       "      <td>b9595cb9d4ef93bd69da</td>\n",
       "      <td>How can I send all Kashmiris to Pakistan? Can Modiji help me?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608883</th>\n",
       "      <td>773aa560c78f08fc3de5</td>\n",
       "      <td>Why did Obama wiretap Trump Tower?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566993</th>\n",
       "      <td>6f16de4d8ed13b7f07bf</td>\n",
       "      <td>Have people become paranoid and prejudice of us autists after what happened with Adam Lanza and Elliot Rodger? Have these two given us autists a bad name? Do people demonize autists because of what these two individuals did especially?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062284</th>\n",
       "      <td>d028778d83046dd07143</td>\n",
       "      <td>What does it mean if my aunt comes up to my room and sits on me when I'm laying down and one time she started sucking my finger what does this mean?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "321903   3f161f8b7be57098b0b6   \n",
       "882246   acd78680501fc908040f   \n",
       "326259   3ff43a2ee7be50e25114   \n",
       "33314    06860ff88c0d7c76fd0a   \n",
       "73556    0e69b32e3da3314aae53   \n",
       "396719   4db993a4950d1c3476ff   \n",
       "945843   b9595cb9d4ef93bd69da   \n",
       "608883   773aa560c78f08fc3de5   \n",
       "566993   6f16de4d8ed13b7f07bf   \n",
       "1062284  d028778d83046dd07143   \n",
       "\n",
       "                                                                                                                                                                                                                                       question_text  \\\n",
       "321903   Why can women never decide what or where they want to eat?                                                                                                                                                                                    \n",
       "882246   Why don't the Turks join the Arabs? How can they turn away from their only brothers?                                                                                                                                                          \n",
       "326259   I am so fucking ugly even you wouldn't date me and I know it? Yo                                                                                                                                                                              \n",
       "33314    What do pro-choice people mean when they talk about the woman's body?                                                                                                                                                                         \n",
       "73556    What would happen if a city or an entire country made humans illegal?                                                                                                                                                                         \n",
       "396719   How do you stop the misinformation spread by pseudo Hindu intellectuals on Quora?                                                                                                                                                             \n",
       "945843   How can I send all Kashmiris to Pakistan? Can Modiji help me?                                                                                                                                                                                 \n",
       "608883   Why did Obama wiretap Trump Tower?                                                                                                                                                                                                            \n",
       "566993   Have people become paranoid and prejudice of us autists after what happened with Adam Lanza and Elliot Rodger? Have these two given us autists a bad name? Do people demonize autists because of what these two individuals did especially?   \n",
       "1062284  What does it mean if my aunt comes up to my room and sits on me when I'm laying down and one time she started sucking my finger what does this mean?                                                                                          \n",
       "\n",
       "         target  \n",
       "321903   1       \n",
       "882246   1       \n",
       "326259   1       \n",
       "33314    1       \n",
       "73556    1       \n",
       "396719   1       \n",
       "945843   1       \n",
       "608883   1       \n",
       "566993   1       \n",
       "1062284  1       "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample insincere questions\")\n",
    "train_df.loc[train_df['target'] == 1].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "n_mslOIdaj8G",
    "outputId": "c0c9da0b-96d8-436f-fe37-28a2da1d6b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sincere questions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>478067</th>\n",
       "      <td>5d9d4b740fef6116975b</td>\n",
       "      <td>Are we here to serve the state/nation or is it here to serve us?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84140</th>\n",
       "      <td>107934b919f1b36cd006</td>\n",
       "      <td>How can I Fix Opera Web Browser Error 72?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441593</th>\n",
       "      <td>568a78bdaa0eb1cfc80a</td>\n",
       "      <td>What was your best experience while having sex?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265734</th>\n",
       "      <td>f80f10f698d0f44f88a2</td>\n",
       "      <td>Is there an open source SD-WAN software package that creates a private VPN network like VeloCloud or Versa?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855810</th>\n",
       "      <td>a7acb495015fabc9e926</td>\n",
       "      <td>What are the most unforgettable bars/pubs in fiction?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680343</th>\n",
       "      <td>853cd314547213fed0b7</td>\n",
       "      <td>Can I install a different desktop environment in Windows?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476604</th>\n",
       "      <td>5d551c56c18a1391abb5</td>\n",
       "      <td>Is Disney making a conscious decision to cast a British brunette woman in every new Star Wars movie?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894306</th>\n",
       "      <td>af3820b92148be03b175</td>\n",
       "      <td>What is the meaning of ensign in the navy?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87524</th>\n",
       "      <td>112779b5a0228f3fd383</td>\n",
       "      <td>What is the meaning of right ventricle dilation? What causes it and what is the treatment?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144263</th>\n",
       "      <td>1c3d7fe77201fdd1a8b1</td>\n",
       "      <td>How does one revert themselves back into being a baby?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "478067   5d9d4b740fef6116975b   \n",
       "84140    107934b919f1b36cd006   \n",
       "441593   568a78bdaa0eb1cfc80a   \n",
       "1265734  f80f10f698d0f44f88a2   \n",
       "855810   a7acb495015fabc9e926   \n",
       "680343   853cd314547213fed0b7   \n",
       "476604   5d551c56c18a1391abb5   \n",
       "894306   af3820b92148be03b175   \n",
       "87524    112779b5a0228f3fd383   \n",
       "144263   1c3d7fe77201fdd1a8b1   \n",
       "\n",
       "                                                                                                       question_text  \\\n",
       "478067   Are we here to serve the state/nation or is it here to serve us?                                              \n",
       "84140    How can I Fix Opera Web Browser Error 72?                                                                     \n",
       "441593   What was your best experience while having sex?                                                               \n",
       "1265734  Is there an open source SD-WAN software package that creates a private VPN network like VeloCloud or Versa?   \n",
       "855810   What are the most unforgettable bars/pubs in fiction?                                                         \n",
       "680343   Can I install a different desktop environment in Windows?                                                     \n",
       "476604   Is Disney making a conscious decision to cast a British brunette woman in every new Star Wars movie?          \n",
       "894306   What is the meaning of ensign in the navy?                                                                    \n",
       "87524    What is the meaning of right ventricle dilation? What causes it and what is the treatment?                    \n",
       "144263   How does one revert themselves back into being a baby?                                                        \n",
       "\n",
       "         target  \n",
       "478067   0       \n",
       "84140    0       \n",
       "441593   0       \n",
       "1265734  0       \n",
       "855810   0       \n",
       "680343   0       \n",
       "476604   0       \n",
       "894306   0       \n",
       "87524    0       \n",
       "144263   0       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample sincere questions\")\n",
    "train_df.loc[train_df['target'] == 0].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.937837\n",
      "1    0.062163\n",
      "Name: target, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff6f224c5f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "target_ratios = train_df.target.value_counts(normalize=True)\n",
    "\n",
    "print(target_ratios)\n",
    "\n",
    "target_ratios.plot(kind='bar', title='Ratios (target)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of questions in train is 13.\n",
      "Average word length of questions in test is 13.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of questions in train is {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of questions in test is {0:.0f}.'.format(np.mean(test_df['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word length of questions in train is 134.\n",
      "Max word length of questions in test is 87.\n"
     ]
    }
   ],
   "source": [
    "print('Max word length of questions in train is {0:.0f}.'.format(np.max(train_df['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Max word length of questions in test is {0:.0f}.'.format(np.max(test_df['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of questions in train is 71.\n",
      "Average character length of questions in test is 70.\n"
     ]
    }
   ],
   "source": [
    "print('Average character length of questions in train is {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x)))))\n",
    "print('Average character length of questions in test is {0:.0f}.'.format(np.mean(test_df['question_text'].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max character length of questions in train is 1017.\n",
      "Max character length of questions in test is 588.\n"
     ]
    }
   ],
   "source": [
    "print('Max character length of questions in train is {0:.0f}.'.format(np.max(train_df['question_text'].apply(lambda x: len(x)))))\n",
    "print('Max character length of questions in test is {0:.0f}.'.format(np.max(test_df['question_text'].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p999 character length of questions in train is 249.\n",
      "p999 character length of questions in test is 249.\n"
     ]
    }
   ],
   "source": [
    "print('p999 character length of questions in train is {0:.0f}.'.format(np.percentile(train_df['question_text'].apply(lambda x: len(x)), 99.9)))\n",
    "print('p999 character length of questions in test is {0:.0f}.'.format(np.percentile(test_df['question_text'].apply(lambda x: len(x)), 99.9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIn-67cmvDFk"
   },
   "source": [
    "## **Preparing the text data**\n",
    "\n",
    "First, we will iterate over the text questions are stored, and format them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bbmOu7FPvNgI",
    "outputId": "653bd496-0185-474a-d557-9d3d2dbacf08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1044897 training questions.\n",
      "Found 261225 validation questions.\n",
      "Found 56370 test questions.\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df['question_text'].fillna('+++').tolist()\n",
    "X_val = val_df['question_text'].fillna('+++').tolist()\n",
    "X_test = test_df['question_text'].fillna('+++').tolist()\n",
    "\n",
    "y_train = train_df['target']\n",
    "y_val = val_df['target']\n",
    "\n",
    "print('Found %s training questions.' % len(X_train))\n",
    "print('Found %s validation questions.' % len(X_val))\n",
    "print('Found %s test questions.' % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_kbWqXCezaBA",
    "outputId": "79004f92-5f67-4ceb-caf0-01a78506cb9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1044897, 250)\n",
      "Shape of y_train: (1044897,)\n",
      "Found 196192 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_WORDS = 100000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, lower=True, split=' ', \n",
    "                       char_level=False, oov_token=None, document_count=0,\n",
    "                      )\n",
    "                                   \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88 ms, sys: 1.18 s, total: 1.26 s\n",
      "Wall time: 6.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pickle\n",
    "\n",
    "test_df.to_pickle('test_df.pkl')\n",
    "\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_val.npy', X_val)\n",
    "np.save('X_test.npy', X_test)\n",
    "\n",
    "y_train.to_pickle('y_train.pkl')\n",
    "y_val.to_pickle('y_val.pkl')\n",
    "\n",
    "pickle.dump(word_index, open('word_index.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "# Oversample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42, ratio = 1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "balanced_train_df = pd.DataFrame()\n",
    "balanced_train_df[0] = np.array(y_train)\n",
    "\n",
    "target_ratios = balanced_train_df[0].value_counts(normalize=True)\n",
    "\n",
    "print(target_ratios)\n",
    "\n",
    "target_ratios.plot(kind='bar', title='Ratios after SMOTE (target)')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "# Setup Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def loadEmbeddings(path, dimensions, mode='r', encoding=None, errors=None):\n",
    "    print('Loading embeddings from: %s' %path)\n",
    "    embeddings = {}\n",
    "    f = open(path, buffering=((2<<16) + 8), mode=mode, encoding=encoding, errors=errors)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-dimensions])\n",
    "        coefs = np.asarray(values[-dimensions:], dtype='float32')\n",
    "        embeddings[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings))\n",
    "    return embeddings\n",
    "\n",
    "def loadEmbeddingsGensim(path, dimensions, binary=True):\n",
    "    print('Loading embeddings from: %s' %path)\n",
    "    embeddings = {}\n",
    "    gensim_vecs = KeyedVectors.load_word2vec_format(path, binary=binary)\n",
    "    for word, vector in zip(gensim_vecs.vocab, gensim_vecs.vectors):\n",
    "        coefs = np.asarray(vector[-dimensions:], dtype='float32')\n",
    "        embeddings[word] = coefs\n",
    "    print('Found %s word vectors.' % len(embeddings))\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KeJJofUpQm9"
   },
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(embedding, word_index, dimensions):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, dimensions))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            if i >= MAX_WORDS:\n",
    "                continue\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfAhNjEJEXmN",
    "outputId": "58618a30-4fab-44f0-8dec-591a8df2da47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: ../input/embeddings/glove.840B.300d/glove.840B.300d.txt\n",
      "Found 2195892 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_path = os.path.join('..', 'input', 'embeddings', 'glove.840B.300d', 'glove.840B.300d.txt')\n",
    "embeddings_index = loadEmbeddings(glove_path, EMBEDDING_DIM)\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index, EMBEDDING_DIM)\n",
    "del embeddings_index\n",
    "np.save('glove.npy', embedding_matrix)\n",
    "embedding_layers['glove'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfAhNjEJEXmN",
    "outputId": "58618a30-4fab-44f0-8dec-591a8df2da47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: ../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\n",
      "Found 1703663 word vectors.\n"
     ]
    }
   ],
   "source": [
    "paragram_path = os.path.join('..', 'input', 'embeddings', 'paragram_300_sl999', 'paragram_300_sl999.txt')\n",
    "embeddings_index = loadEmbeddings(paragram_path, EMBEDDING_DIM, encoding='utf8', errors='ignore')\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index, EMBEDDING_DIM)\n",
    "del embeddings_index\n",
    "np.save('paragram.npy', embedding_matrix)\n",
    "embedding_layers['paragram'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfAhNjEJEXmN",
    "outputId": "58618a30-4fab-44f0-8dec-591a8df2da47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: ../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\n",
      "Found 999995 word vectors.\n"
     ]
    }
   ],
   "source": [
    "wiki_path = os.path.join('..', 'input', 'embeddings', 'wiki-news-300d-1M', 'wiki-news-300d-1M.vec')\n",
    "embeddings_index = loadEmbeddings(wiki_path, EMBEDDING_DIM)\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index, EMBEDDING_DIM)\n",
    "del embeddings_index\n",
    "np.save('wiki.npy', embedding_matrix)\n",
    "embedding_layers['wiki'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: ../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\n",
      "Found 3000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "google_news_path = os.path.join('..', 'input', 'embeddings', 'GoogleNews-vectors-negative300', 'GoogleNews-vectors-negative300.bin')\n",
    "embeddings_index = loadEmbeddingsGensim(google_news_path, EMBEDDING_DIM)\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index, EMBEDDING_DIM)\n",
    "del embeddings_index\n",
    "np.save('google_news.npy', embedding_matrix)\n",
    "embedding_layers['google_news'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76 ms, sys: 1.11 s, total: 1.19 s\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "test_df = pd.read_pickle('test_df.pkl')\n",
    "\n",
    "X_train = np.load('X_train.npy')\n",
    "X_val = np.load('X_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "\n",
    "y_train = pd.read_pickle('y_train.pkl')\n",
    "y_val = pd.read_pickle('y_val.pkl')\n",
    "\n",
    "word_index = pickle.load(open('word_index.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44 ms, sys: 1.54 s, total: 1.58 s\n",
      "Wall time: 7.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embedding_matrix = np.load('glove.npy')\n",
    "embedding_layers['glove'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "embedding_matrix = np.load('paragram.npy')\n",
    "embedding_layers['paragram'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "embedding_matrix = np.load('wiki.npy')\n",
    "embedding_layers['wiki'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "embedding_matrix = np.load('google_news.npy')\n",
    "embedding_layers['google_news'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNd1KcNjsvxj"
   },
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 250, 300)     58857900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 250, 300)     58857900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 250, 300)     58857900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 250, 300)     58857900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 250, 128)     140544      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 250, 128)     140544      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 250, 128)     140544      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 250, 128)     140544      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2048        global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           2048        global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2048        global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           2048        global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16)           64          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           64          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16)           64          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16)           64          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16)           0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2048        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32)           128         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            32          activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1)            4           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1)            0           batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 236,004,436\n",
      "Trainable params: 572,642\n",
      "Non-trainable params: 235,431,794\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "CPU times: user 8.85 s, sys: 4.86 s, total: 13.7 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from keras.layers import Dense, Dropout, Input, GlobalMaxPool1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Bidirectional\n",
    "from keras.layers import Activation, BatchNormalization, CuDNNGRU\n",
    "from keras.layers import SpatialDropout1D, Concatenate, Flatten, Reshape\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "a = embedding_layers['glove'] (inp)\n",
    "a = Bidirectional(CuDNNGRU(64, return_sequences=True))(a)\n",
    "a = GlobalMaxPool1D()(a)\n",
    "a = Dense(16, use_bias=False)(a)\n",
    "a = BatchNormalization()(a)\n",
    "a = Activation(\"relu\")(a)\n",
    "a = Dropout(0.1)(a)\n",
    "\n",
    "b = embedding_layers['paragram'] (inp)\n",
    "b = Bidirectional(CuDNNGRU(64, return_sequences=True))(b)\n",
    "b = GlobalMaxPool1D()(b)\n",
    "b = Dense(16, use_bias=False)(b)\n",
    "b = BatchNormalization()(b)\n",
    "b = Activation(\"relu\")(b)\n",
    "b = Dropout(0.1)(b)\n",
    "\n",
    "c = embedding_layers['wiki'] (inp)\n",
    "c = Bidirectional(CuDNNGRU(64, return_sequences=True))(c)\n",
    "c = GlobalMaxPool1D()(c)\n",
    "c = Dense(16, use_bias=False)(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Activation(\"relu\")(c)\n",
    "c = Dropout(0.1)(c)\n",
    "\n",
    "d = embedding_layers['google_news'] (inp)\n",
    "d = Bidirectional(CuDNNGRU(64, return_sequences=True))(d)\n",
    "d = GlobalMaxPool1D()(d)\n",
    "d = Dense(16, use_bias=False)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = Activation(\"relu\")(d)\n",
    "d = Dropout(0.1)(d)\n",
    "\n",
    "x = Concatenate(axis=1)([a, b, c, d])\n",
    "x = Dense(32, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "#kernel_regularizer=l2(0.01)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dense(1, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "out = Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lp4o2h9Fs8gP"
   },
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L75Rc0Gqtclm"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "Use model checkpointing to save the model that attains the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3OwX8ErsteZL",
    "outputId": "f718108c-5f78-4d9b-8814-c7044a9cce96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/2\n",
      "1044897/1044897 [==============================] - 517s 495us/step - loss: 0.3668 - acc: 0.9366 - val_loss: 0.2188 - val_acc: 0.9562\n",
      "Epoch 2/2\n",
      "1044897/1044897 [==============================] - 517s 495us/step - loss: 0.1624 - acc: 0.9600 - val_loss: 0.1284 - val_acc: 0.9608\n",
      "CPU times: user 13min 12s, sys: 2min 6s, total: 15min 18s\n",
      "Wall time: 17min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc9144c4e48>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=2, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# serialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# serialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model weights to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"weights.h5\")\n",
    "print(\"Saved model weights to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Predict validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261225/261225 [==============================] - 30s 114us/step\n",
      "CPU times: user 27.6 s, sys: 5.82 s, total: 33.4 s\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_val = model.predict([X_val], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.3100 with F1 score: 0.6713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def bestThreshold(y_true,y_pred):\n",
    "    idx = 0\n",
    "    cur_f1 = 0\n",
    "    max_f1 = 0\n",
    "    thres = 0\n",
    "    for idx in np.arange(0.1, 0.501, 0.01):\n",
    "        cur_f1 = f1_score(y_true, np.array(y_pred)> idx)\n",
    "        if cur_f1 > max_f1:\n",
    "            max_f1 = cur_f1\n",
    "            thres = idx\n",
    "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(thres, max_f1))\n",
    "    return thres\n",
    "threshold = bestThreshold(y_val,pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Predict test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370/56370 [==============================] - 6s 114us/step\n",
      "CPU times: user 5.68 s, sys: 1.49 s, total: 7.17 s\n",
      "Wall time: 6.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_test = model.predict([X_test], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "submission_df['prediction'] = (pred_test > threshold).astype(int)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "capstone.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
