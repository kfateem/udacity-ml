{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7oagxHHofxb"
   },
   "source": [
    "# Import dataset in to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3n8bRW6WcKo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7oagxHHofxb"
   },
   "source": [
    "# Analyze train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "h0aRi6Toaq8j",
    "outputId": "5d5c6246-c8c3-49e0-e8b7-58850a83f62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample insincere questions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1169030</th>\n",
       "      <td>e5131ef6e7108b499072</td>\n",
       "      <td>Why do Russians lie about their history, what they have invented, etc.?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857155</th>\n",
       "      <td>a7f0cab9fbf793e3c47b</td>\n",
       "      <td>Why do Americans usually get pissed off when you speak a different language in front of them, even when you're not in the U.S?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62415</th>\n",
       "      <td>0c409518d04ea447d03a</td>\n",
       "      <td>Why doesn't Brazilians, argentinos and uruguaios go back to their country?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759858</th>\n",
       "      <td>94e12c23ff0c28a2aaa3</td>\n",
       "      <td>In Christianity, if we can just forgive everybody, than why don’t Christians follow this?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103894</th>\n",
       "      <td>d858a5204928bab5f854</td>\n",
       "      <td>I nearly suffocated whilst travelling on a coach full of East Asians in the UK with all sunroofs closed. Is this the \"bad air\" people talk of about East Asians?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986312</th>\n",
       "      <td>c13a592453d77bfdcf04</td>\n",
       "      <td>Why all these idiots that are against Republic of Macedonia don't get in hell?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842101</th>\n",
       "      <td>a50be202c1bdc5950cbe</td>\n",
       "      <td>Are American gays mentally ill- it seems they are misogynists who can't get along with straight people?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803619</th>\n",
       "      <td>9d76383d3019e92d2f19</td>\n",
       "      <td>How common is for hillbillies to marry latinos?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827112</th>\n",
       "      <td>a218913182e67d5beb45</td>\n",
       "      <td>If Queen Elizabeth demanded that all Canadians jump, would they be obliged to do so?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191285</th>\n",
       "      <td>2565f633aa37c9feefbb</td>\n",
       "      <td>How do I get girls to like me and also how do I use chloroform?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "1169030  e5131ef6e7108b499072   \n",
       "857155   a7f0cab9fbf793e3c47b   \n",
       "62415    0c409518d04ea447d03a   \n",
       "759858   94e12c23ff0c28a2aaa3   \n",
       "1103894  d858a5204928bab5f854   \n",
       "986312   c13a592453d77bfdcf04   \n",
       "842101   a50be202c1bdc5950cbe   \n",
       "803619   9d76383d3019e92d2f19   \n",
       "827112   a218913182e67d5beb45   \n",
       "191285   2565f633aa37c9feefbb   \n",
       "\n",
       "                                                                                                                                                            question_text  \\\n",
       "1169030  Why do Russians lie about their history, what they have invented, etc.?                                                                                            \n",
       "857155   Why do Americans usually get pissed off when you speak a different language in front of them, even when you're not in the U.S?                                     \n",
       "62415    Why doesn't Brazilians, argentinos and uruguaios go back to their country?                                                                                         \n",
       "759858   In Christianity, if we can just forgive everybody, than why don’t Christians follow this?                                                                          \n",
       "1103894  I nearly suffocated whilst travelling on a coach full of East Asians in the UK with all sunroofs closed. Is this the \"bad air\" people talk of about East Asians?   \n",
       "986312   Why all these idiots that are against Republic of Macedonia don't get in hell?                                                                                     \n",
       "842101   Are American gays mentally ill- it seems they are misogynists who can't get along with straight people?                                                            \n",
       "803619   How common is for hillbillies to marry latinos?                                                                                                                    \n",
       "827112   If Queen Elizabeth demanded that all Canadians jump, would they be obliged to do so?                                                                               \n",
       "191285   How do I get girls to like me and also how do I use chloroform?                                                                                                    \n",
       "\n",
       "         target  \n",
       "1169030  1       \n",
       "857155   1       \n",
       "62415    1       \n",
       "759858   1       \n",
       "1103894  1       \n",
       "986312   1       \n",
       "842101   1       \n",
       "803619   1       \n",
       "827112   1       \n",
       "191285   1       "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample insincere questions\")\n",
    "train_df.loc[train_df['target'] == 1].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "n_mslOIdaj8G",
    "outputId": "c0c9da0b-96d8-436f-fe37-28a2da1d6b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sincere questions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>788248</th>\n",
       "      <td>9a6dcb5857d9d27d6dc6</td>\n",
       "      <td>What are some fun facts about Neptune?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775641</th>\n",
       "      <td>97ee8141d0c489059673</td>\n",
       "      <td>Is it safe to take my Bernese Mountain dog for walks if he's only 4 months old?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122244</th>\n",
       "      <td>dbe5a7029309068f22b3</td>\n",
       "      <td>I will be driving from San Diego to Montreal in about 10 days - what are some ways I keep eating healthy?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158744</th>\n",
       "      <td>e30b891c43bbe788c6df</td>\n",
       "      <td>What's it like living in India? How's the cost of living? Is it safe or is there a lot of crime? Do they have grocery stores or do you get food from markets? Is it hard to live there if you only speak English?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189773</th>\n",
       "      <td>e92744c2fed80885c8e2</td>\n",
       "      <td>Where is ladies service bar in Mumbai?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230049</th>\n",
       "      <td>f10b7e44414dbb25f175</td>\n",
       "      <td>Why my penis bleed on first sex?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720603</th>\n",
       "      <td>8d104b4652977e36876e</td>\n",
       "      <td>We have Armed security guards in shopping malls. Why is it such a horrible idea to have a few armed guards at high schools?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288131</th>\n",
       "      <td>386e9ade298d72e0cf37</td>\n",
       "      <td>What are the best online part time jobs for a polymer engineer?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364486</th>\n",
       "      <td>47742984713bd27d546e</td>\n",
       "      <td>Where can I find a support group in the US for people with stage 4 lung cancer?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571030</th>\n",
       "      <td>6fe479fc0c17eb1c2897</td>\n",
       "      <td>What significance did the song that plays in Pete and Pete's opening credits have?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "788248   9a6dcb5857d9d27d6dc6   \n",
       "775641   97ee8141d0c489059673   \n",
       "1122244  dbe5a7029309068f22b3   \n",
       "1158744  e30b891c43bbe788c6df   \n",
       "1189773  e92744c2fed80885c8e2   \n",
       "1230049  f10b7e44414dbb25f175   \n",
       "720603   8d104b4652977e36876e   \n",
       "288131   386e9ade298d72e0cf37   \n",
       "364486   47742984713bd27d546e   \n",
       "571030   6fe479fc0c17eb1c2897   \n",
       "\n",
       "                                                                                                                                                                                                             question_text  \\\n",
       "788248   What are some fun facts about Neptune?                                                                                                                                                                              \n",
       "775641   Is it safe to take my Bernese Mountain dog for walks if he's only 4 months old?                                                                                                                                     \n",
       "1122244  I will be driving from San Diego to Montreal in about 10 days - what are some ways I keep eating healthy?                                                                                                           \n",
       "1158744  What's it like living in India? How's the cost of living? Is it safe or is there a lot of crime? Do they have grocery stores or do you get food from markets? Is it hard to live there if you only speak English?   \n",
       "1189773  Where is ladies service bar in Mumbai?                                                                                                                                                                              \n",
       "1230049  Why my penis bleed on first sex?                                                                                                                                                                                    \n",
       "720603   We have Armed security guards in shopping malls. Why is it such a horrible idea to have a few armed guards at high schools?                                                                                         \n",
       "288131   What are the best online part time jobs for a polymer engineer?                                                                                                                                                     \n",
       "364486   Where can I find a support group in the US for people with stage 4 lung cancer?                                                                                                                                     \n",
       "571030   What significance did the song that plays in Pete and Pete's opening credits have?                                                                                                                                  \n",
       "\n",
       "         target  \n",
       "788248   0       \n",
       "775641   0       \n",
       "1122244  0       \n",
       "1158744  0       \n",
       "1189773  0       \n",
       "1230049  0       \n",
       "720603   0       \n",
       "288131   0       \n",
       "364486   0       \n",
       "571030   0       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample sincere questions\")\n",
    "train_df.loc[train_df['target'] == 0].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.937946\n",
      "1    0.062054\n",
      "Name: target, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8487f0d5f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "target_ratios = train_df.target.value_counts(normalize=True)\n",
    "\n",
    "print(target_ratios)\n",
    "\n",
    "target_ratios.plot(kind='bar', title='Ratios (target)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of questions in train is 13.\n",
      "Average word length of questions in test is 13.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of questions in train is {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of questions in test is {0:.0f}.'.format(np.mean(test_df['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word length of questions in train is 134.\n",
      "Max word length of questions in test is 87.\n"
     ]
    }
   ],
   "source": [
    "print('Max word length of questions in train is {0:.0f}.'.format(np.max(train_df['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Max word length of questions in test is {0:.0f}.'.format(np.max(test_df['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of questions in train is 71.\n",
      "Average character length of questions in test is 70.\n"
     ]
    }
   ],
   "source": [
    "print('Average character length of questions in train is {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x)))))\n",
    "print('Average character length of questions in test is {0:.0f}.'.format(np.mean(test_df['question_text'].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max character length of questions in train is 1017.\n",
      "Max character length of questions in test is 588.\n"
     ]
    }
   ],
   "source": [
    "print('Max character length of questions in train is {0:.0f}.'.format(np.max(train_df['question_text'].apply(lambda x: len(x)))))\n",
    "print('Max character length of questions in test is {0:.0f}.'.format(np.max(test_df['question_text'].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p999 character length of questions in train is 249.\n",
      "p999 character length of questions in test is 249.\n"
     ]
    }
   ],
   "source": [
    "print('p999 character length of questions in train is {0:.0f}.'.format(np.percentile(train_df['question_text'].apply(lambda x: len(x)), 99.9)))\n",
    "print('p999 character length of questions in test is {0:.0f}.'.format(np.percentile(test_df['question_text'].apply(lambda x: len(x)), 99.9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIn-67cmvDFk"
   },
   "source": [
    "## **Preparing the text data**\n",
    "\n",
    "First, we will iterate over the text questions are stored, and format them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bbmOu7FPvNgI",
    "outputId": "653bd496-0185-474a-d557-9d3d2dbacf08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1175509 training questions.\n",
      "Found 130613 validation questions.\n",
      "Found 56370 test questions.\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df['question_text'].fillna('+++').tolist()\n",
    "X_val = val_df['question_text'].fillna('+++').tolist()\n",
    "X_test = test_df['question_text'].fillna('+++').tolist()\n",
    "\n",
    "y_train = train_df['target']\n",
    "y_val = val_df['target']\n",
    "\n",
    "print('Found %s training questions.' % len(X_train))\n",
    "print('Found %s validation questions.' % len(X_val))\n",
    "print('Found %s test questions.' % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_kbWqXCezaBA",
    "outputId": "79004f92-5f67-4ceb-caf0-01a78506cb9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1175509, 250)\n",
      "Shape of y_train: (1175509,)\n",
      "Found 209541 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_WORDS = 100000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, lower=True, split=' ', \n",
    "                       char_level=False, oov_token=None, document_count=0,\n",
    "                      )\n",
    "                                   \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "# Oversample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42, ratio = 1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0.5\n",
      "0    0.5\n",
      "Name: 0, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f841801cfd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "balanced_train_df = pd.DataFrame()\n",
    "balanced_train_df[0] = np.array(y_train)\n",
    "\n",
    "target_ratios = balanced_train_df[0].value_counts(normalize=True)\n",
    "\n",
    "print(target_ratios)\n",
    "\n",
    "target_ratios.plot(kind='bar', title='Ratios after SMOTE (target)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "# Setup Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def loadEmbeddings(path, dimensions, mode='r', encoding=None, errors=None):\n",
    "    print('Loading embeddings from: %s' %path)\n",
    "    embeddings = {}\n",
    "    f = open(path, buffering=((2<<16) + 8), mode=mode, encoding=encoding, errors=errors)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-dimensions])\n",
    "        coefs = np.asarray(values[-dimensions:], dtype='float32')\n",
    "        embeddings[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings))\n",
    "    return embeddings\n",
    "\n",
    "def loadEmbeddingsGensim(path, dimensions, binary=True):\n",
    "    print('Loading embeddings from: %s' %path)\n",
    "    embeddings = {}\n",
    "    gensim_vecs = KeyedVectors.load_word2vec_format(path, binary=binary)\n",
    "    for word, vector in zip(gensim_vecs.vocab, gensim_vecs.vectors):\n",
    "        coefs = np.asarray(vector[-dimensions:], dtype='float32')\n",
    "        embeddings[word] = coefs\n",
    "    print('Found %s word vectors.' % len(embeddings))\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KeJJofUpQm9"
   },
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(embedding, word_index, dimensions):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, dimensions))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            if i >= MAX_WORDS:\n",
    "                continue\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfAhNjEJEXmN",
    "outputId": "58618a30-4fab-44f0-8dec-591a8df2da47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: ../input/embeddings/glove.840B.300d/glove.840B.300d.txt\n",
      "Found 2195892 word vectors.\n",
      "peak memory: 5765.53 MiB, increment: 0.65 MiB\n"
     ]
    }
   ],
   "source": [
    "glove_path = os.path.join('..', 'input', 'embeddings', 'glove.840B.300d', 'glove.840B.300d.txt')\n",
    "embeddings_index = loadEmbeddings(glove_path, EMBEDDING_DIM)\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index, EMBEDDING_DIM)\n",
    "del embeddings_index\n",
    "np.save('glove.npy', embedding_matrix)\n",
    "embedding_layers['glove'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfAhNjEJEXmN",
    "outputId": "58618a30-4fab-44f0-8dec-591a8df2da47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: ../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\n",
      "Found 1703663 word vectors.\n",
      "peak memory: 5080.59 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "paragram_path = os.path.join('..', 'input', 'embeddings', 'paragram_300_sl999', 'paragram_300_sl999.txt')\n",
    "embeddings_index = loadEmbeddings(paragram_path, EMBEDDING_DIM, encoding='utf8', errors='ignore')\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index, EMBEDDING_DIM)\n",
    "del embeddings_index\n",
    "np.save('paragram.npy', embedding_matrix)\n",
    "embedding_layers['paragram'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfAhNjEJEXmN",
    "outputId": "58618a30-4fab-44f0-8dec-591a8df2da47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: ../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\n",
      "Found 999995 word vectors.\n",
      "peak memory: 5272.27 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "wiki_path = os.path.join('..', 'input', 'embeddings', 'wiki-news-300d-1M', 'wiki-news-300d-1M.vec')\n",
    "embeddings_index = loadEmbeddings(wiki_path, EMBEDDING_DIM)\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index, EMBEDDING_DIM)\n",
    "del embeddings_index\n",
    "np.save('wiki.npy', embedding_matrix)\n",
    "embedding_layers['wiki'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from: ../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\n",
      "Found 3000000 word vectors.\n",
      "peak memory: 5463.97 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "google_news_path = os.path.join('..', 'input', 'embeddings', 'GoogleNews-vectors-negative300', 'GoogleNews-vectors-negative300.bin')\n",
    "embeddings_index = loadEmbeddingsGensim(google_news_path, EMBEDDING_DIM)\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index, EMBEDDING_DIM)\n",
    "del embeddings_index\n",
    "np.save('google_news.npy', embedding_matrix)\n",
    "embedding_layers['google_news'] = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5462.97 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit foo = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNd1KcNjsvxj"
   },
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 250, 300)     62862600    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 250, 300)     62862600    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 250, 300)     62862600    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_28 (Embedding)        (None, 250, 300)     62862600    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 250, 128)     140544      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 250, 128)     140544      embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 250, 128)     140544      embedding_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 250, 128)     140544      embedding_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2048        global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           2048        global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2048        global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           2048        global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16)           64          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           64          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16)           64          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16)           64          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16)           0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4096        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64)           256         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            64          activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1)            4           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1)            0           batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 252,025,444\n",
      "Trainable params: 574,786\n",
      "Non-trainable params: 251,450,658\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "CPU times: user 10.5 s, sys: 2.8 s, total: 13.3 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from keras.layers import Dense, Dropout, Input, GlobalMaxPool1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Bidirectional\n",
    "from keras.layers import Activation, BatchNormalization, CuDNNGRU\n",
    "from keras.layers import SpatialDropout1D, Concatenate, Flatten, Reshape\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "a = embedding_layers['glove'] (inp)\n",
    "a = Bidirectional(CuDNNGRU(64, return_sequences=True))(a)\n",
    "a = GlobalMaxPool1D()(a)\n",
    "a = Dense(16, use_bias=False)(a)\n",
    "a = BatchNormalization()(a)\n",
    "a = Activation(\"relu\")(a)\n",
    "a = Dropout(0.1)(a)\n",
    "\n",
    "b = embedding_layers['paragram'] (inp)\n",
    "b = Bidirectional(CuDNNGRU(64, return_sequences=True))(b)\n",
    "b = GlobalMaxPool1D()(b)\n",
    "b = Dense(16, use_bias=False)(b)\n",
    "b = BatchNormalization()(b)\n",
    "b = Activation(\"relu\")(b)\n",
    "b = Dropout(0.1)(b)\n",
    "\n",
    "c = embedding_layers['wiki'] (inp)\n",
    "c = Bidirectional(CuDNNGRU(64, return_sequences=True))(c)\n",
    "c = GlobalMaxPool1D()(c)\n",
    "c = Dense(16, use_bias=False)(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Activation(\"relu\")(c)\n",
    "c = Dropout(0.1)(c)\n",
    "\n",
    "d = embedding_layers['google_news'] (inp)\n",
    "d = Bidirectional(CuDNNGRU(64, return_sequences=True))(d)\n",
    "d = GlobalMaxPool1D()(d)\n",
    "d = Dense(16, use_bias=False)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = Activation(\"relu\")(d)\n",
    "d = Dropout(0.1)(d)\n",
    "\n",
    "x = Concatenate(axis=1)([a, b, c, d])\n",
    "x = Dense(64, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "#kernel_regularizer=l2(0.01)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dense(1, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "out = Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 250, 300)     62862600    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 250, 300)     62862600    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 250, 300)     62862600    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_28 (Embedding)        (None, 250, 300)     62862600    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 250, 128)     140544      embedding_7[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 250, 128)     140544      embedding_14[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 250, 128)     140544      embedding_21[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 250, 128)     140544      embedding_28[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           2048        global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           2048        global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16)           2048        global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           2048        global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16)           64          dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16)           64          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16)           64          dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16)           64          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16)           0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16)           0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16)           0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16)           0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16)           0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 16)           0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 16)           0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 16)           0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64)           0           dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 32)           2048        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32)           128         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32)           0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            32          activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 1)            4           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 1)            0           batch_normalization_12[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 252,023,236\n",
      "Trainable params: 572,642\n",
      "Non-trainable params: 251,450,594\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "CPU times: user 2.28 s, sys: 1.41 s, total: 3.69 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from keras.layers import Dense, Dropout, Input, GlobalMaxPool1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Bidirectional\n",
    "from keras.layers import Activation, BatchNormalization, CuDNNGRU\n",
    "from keras.layers import SpatialDropout1D, Concatenate, Flatten, Reshape\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "a = embedding_layers['glove'] (inp)\n",
    "a = Bidirectional(CuDNNGRU(64, return_sequences=True))(a)\n",
    "a = GlobalMaxPool1D()(a)\n",
    "a = Dense(16, use_bias=False)(a)\n",
    "a = BatchNormalization()(a)\n",
    "a = Activation(\"relu\")(a)\n",
    "a = Dropout(0.1)(a)\n",
    "\n",
    "b = embedding_layers['paragram'] (inp)\n",
    "b = Bidirectional(CuDNNGRU(64, return_sequences=True))(b)\n",
    "b = GlobalMaxPool1D()(b)\n",
    "b = Dense(16, use_bias=False)(b)\n",
    "b = BatchNormalization()(b)\n",
    "b = Activation(\"relu\")(b)\n",
    "b = Dropout(0.1)(b)\n",
    "\n",
    "c = embedding_layers['wiki'] (inp)\n",
    "c = Bidirectional(CuDNNGRU(64, return_sequences=True))(c)\n",
    "c = GlobalMaxPool1D()(c)\n",
    "c = Dense(16, use_bias=False)(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Activation(\"relu\")(c)\n",
    "c = Dropout(0.1)(c)\n",
    "\n",
    "d = embedding_layers['google_news'] (inp)\n",
    "d = Bidirectional(CuDNNGRU(64, return_sequences=True))(d)\n",
    "d = GlobalMaxPool1D()(d)\n",
    "d = Dense(16, use_bias=False)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = Activation(\"relu\")(d)\n",
    "d = Dropout(0.1)(d)\n",
    "\n",
    "x = Concatenate(axis=1)([a, b, c, d])\n",
    "x = Dense(32, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "#kernel_regularizer=l2(0.01)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dense(1, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "out = Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 250, 300)     62862600    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 250, 300)     62862600    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 250, 300)     62862600    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_28 (Embedding)        (None, 250, 300)     62862600    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 250, 128)     140544      embedding_7[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 250, 128)     140544      embedding_14[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 250, 128)     140544      embedding_21[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 250, 128)     140544      embedding_28[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 16)           2048        global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 16)           2048        global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 16)           2048        global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 16)           2048        global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16)           64          dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16)           64          dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16)           64          dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16)           64          dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16)           0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16)           0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16)           0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16)           0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16)           0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16)           0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16)           0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16)           0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (Average)             (None, 16)           0           dropout_9[0][0]                  \n",
      "                                                                 dropout_10[0][0]                 \n",
      "                                                                 dropout_11[0][0]                 \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 32)           512         average_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32)           128         dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32)           0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1)            32          activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 1)            4           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 1)            0           batch_normalization_18[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 252,021,700\n",
      "Trainable params: 571,106\n",
      "Non-trainable params: 251,450,594\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "CPU times: user 1.85 s, sys: 1.5 s, total: 3.35 s\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from keras.layers import Dense, Dropout, Input, GlobalMaxPool1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Bidirectional\n",
    "from keras.layers import Activation, BatchNormalization, CuDNNGRU\n",
    "from keras.layers import SpatialDropout1D, Average, Flatten, Reshape\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "a = embedding_layers['glove'] (inp)\n",
    "a = Bidirectional(CuDNNGRU(64, return_sequences=True))(a)\n",
    "a = GlobalMaxPool1D()(a)\n",
    "a = Dense(16, use_bias=False)(a)\n",
    "a = BatchNormalization()(a)\n",
    "a = Activation(\"relu\")(a)\n",
    "a = Dropout(0.1)(a)\n",
    "\n",
    "b = embedding_layers['paragram'] (inp)\n",
    "b = Bidirectional(CuDNNGRU(64, return_sequences=True))(b)\n",
    "b = GlobalMaxPool1D()(b)\n",
    "b = Dense(16, use_bias=False)(b)\n",
    "b = BatchNormalization()(b)\n",
    "b = Activation(\"relu\")(b)\n",
    "b = Dropout(0.1)(b)\n",
    "\n",
    "c = embedding_layers['wiki'] (inp)\n",
    "c = Bidirectional(CuDNNGRU(64, return_sequences=True))(c)\n",
    "c = GlobalMaxPool1D()(c)\n",
    "c = Dense(16, use_bias=False)(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Activation(\"relu\")(c)\n",
    "c = Dropout(0.1)(c)\n",
    "\n",
    "d = embedding_layers['google_news'] (inp)\n",
    "d = Bidirectional(CuDNNGRU(64, return_sequences=True))(d)\n",
    "d = GlobalMaxPool1D()(d)\n",
    "d = Dense(16, use_bias=False)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = Activation(\"relu\")(d)\n",
    "d = Dropout(0.1)(d)\n",
    "\n",
    "x = Average()([a, b, c, d])\n",
    "x = Dense(32, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "#kernel_regularizer=l2(0.01)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dense(1, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "out = Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lp4o2h9Fs8gP"
   },
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L75Rc0Gqtclm"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "Use model checkpointing to save the model that attains the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3OwX8ErsteZL",
    "outputId": "f718108c-5f78-4d9b-8814-c7044a9cce96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2205128 samples, validate on 130613 samples\n",
      "Epoch 1/2\n",
      "2205128/2205128 [==============================] - 681s 309us/step - loss: 0.2027 - acc: 0.9592 - val_loss: 0.1646 - val_acc: 0.9498\n",
      "Epoch 2/2\n",
      "2205128/2205128 [==============================] - 674s 306us/step - loss: 0.0953 - acc: 0.9728 - val_loss: 0.1267 - val_acc: 0.9538\n",
      "CPU times: user 16min 10s, sys: 2min 52s, total: 19min 3s\n",
      "Wall time: 22min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84157225f8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=2, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# serialize model to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# serialize weights to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model weights to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"weights.h5\")\n",
    "print(\"Saved model weights to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Predict validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130613/130613 [==============================] - 15s 113us/step\n",
      "CPU times: user 13.7 s, sys: 2.86 s, total: 16.6 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_val = model.predict([X_val], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.3900 with F1 score: 0.6244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def bestThreshold(y_true,y_pred):\n",
    "    idx = 0\n",
    "    cur_f1 = 0\n",
    "    max_f1 = 0\n",
    "    thres = 0\n",
    "    for idx in np.arange(0.1, 0.501, 0.01):\n",
    "        cur_f1 = f1_score(y_true, np.array(y_pred)> idx)\n",
    "        if cur_f1 > max_f1:\n",
    "            max_f1 = cur_f1\n",
    "            thres = idx\n",
    "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(thres, max_f1))\n",
    "    return thres\n",
    "threshold = bestThreshold(y_val,pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Predict test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370/56370 [==============================] - 6s 110us/step\n",
      "CPU times: user 5.74 s, sys: 1.18 s, total: 6.92 s\n",
      "Wall time: 6.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_test = model.predict([X_test], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "submission_df['prediction'] = (pred_test > threshold).astype(int)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "capstone.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
