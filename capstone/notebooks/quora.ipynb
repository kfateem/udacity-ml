{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7oagxHHofxb"
   },
   "source": [
    "# Import dataset in to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3n8bRW6WcKo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "h0aRi6Toaq8j",
    "outputId": "5d5c6246-c8c3-49e0-e8b7-58850a83f62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample insincere questions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>376293</th>\n",
       "      <td>49c3c879eeb35b7711b2</td>\n",
       "      <td>Don't democrats realize that the progressive ideology has destroyed the Democratic Party?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232529</th>\n",
       "      <td>f18835e3747e515ac34b</td>\n",
       "      <td>If there is an endless supply of men, especially online and there are beta men that don't mind accepting even unattractive or obese women, how come some women end up involuntarily single for life?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014656</th>\n",
       "      <td>c6d56def6d129abb3135</td>\n",
       "      <td>Do French people in general have such bigoted views towards Syrian rebels?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107771</th>\n",
       "      <td>d915f251921f68ab864c</td>\n",
       "      <td>Why do Persians believe in 'white Aryan Indo-European' Nazi ideology?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127449</th>\n",
       "      <td>dcf1be6c98286df68999</td>\n",
       "      <td>Are Chinese people answering on Quora paid by the CCP in an attempt to shape people’s opinions about the CCP?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854190</th>\n",
       "      <td>a75bffebce2c8c7a9432</td>\n",
       "      <td>When would India become Muslim free?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428435</th>\n",
       "      <td>53f8d84a4d646b427c8f</td>\n",
       "      <td>Do atheists honestly believe that Darwin's anti-religious bigoted family would have admitted that he had a deathbed conversion?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186254</th>\n",
       "      <td>e8789e001d6f5e8ccdeb</td>\n",
       "      <td>Why are there so many stupid people on Quora. and why do you stupid teenagers not Google this shit?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964524</th>\n",
       "      <td>bcf85db9b95f1b012df2</td>\n",
       "      <td>How are dominant groups carrying out racism in America? And what reasons do the dominant group use to justify their racism?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215250</th>\n",
       "      <td>ee2d665a2a19810e8d52</td>\n",
       "      <td>Why are liberals viewed as anti-authority when they are always going “there ought to be a law” on this and that, and end up regulating/criminalizing more and more day to day behaviors?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "376293   49c3c879eeb35b7711b2   \n",
       "1232529  f18835e3747e515ac34b   \n",
       "1014656  c6d56def6d129abb3135   \n",
       "1107771  d915f251921f68ab864c   \n",
       "1127449  dcf1be6c98286df68999   \n",
       "854190   a75bffebce2c8c7a9432   \n",
       "428435   53f8d84a4d646b427c8f   \n",
       "1186254  e8789e001d6f5e8ccdeb   \n",
       "964524   bcf85db9b95f1b012df2   \n",
       "1215250  ee2d665a2a19810e8d52   \n",
       "\n",
       "                                                                                                                                                                                                question_text  \\\n",
       "376293   Don't democrats realize that the progressive ideology has destroyed the Democratic Party?                                                                                                              \n",
       "1232529  If there is an endless supply of men, especially online and there are beta men that don't mind accepting even unattractive or obese women, how come some women end up involuntarily single for life?   \n",
       "1014656  Do French people in general have such bigoted views towards Syrian rebels?                                                                                                                             \n",
       "1107771  Why do Persians believe in 'white Aryan Indo-European' Nazi ideology?                                                                                                                                  \n",
       "1127449  Are Chinese people answering on Quora paid by the CCP in an attempt to shape people’s opinions about the CCP?                                                                                          \n",
       "854190   When would India become Muslim free?                                                                                                                                                                   \n",
       "428435   Do atheists honestly believe that Darwin's anti-religious bigoted family would have admitted that he had a deathbed conversion?                                                                        \n",
       "1186254  Why are there so many stupid people on Quora. and why do you stupid teenagers not Google this shit?                                                                                                    \n",
       "964524   How are dominant groups carrying out racism in America? And what reasons do the dominant group use to justify their racism?                                                                            \n",
       "1215250  Why are liberals viewed as anti-authority when they are always going “there ought to be a law” on this and that, and end up regulating/criminalizing more and more day to day behaviors?               \n",
       "\n",
       "         target  \n",
       "376293   1       \n",
       "1232529  1       \n",
       "1014656  1       \n",
       "1107771  1       \n",
       "1127449  1       \n",
       "854190   1       \n",
       "428435   1       \n",
       "1186254  1       \n",
       "964524   1       \n",
       "1215250  1       "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample insincere questions\")\n",
    "train_df.loc[train_df['target'] == 1].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "n_mslOIdaj8G",
    "outputId": "c0c9da0b-96d8-436f-fe37-28a2da1d6b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sincere questions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244246</th>\n",
       "      <td>2fc50134cb76b8b254af</td>\n",
       "      <td>After bankruptcy do you sill have to pay liabilities?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114457</th>\n",
       "      <td>da61e1f4461d6a03ddfc</td>\n",
       "      <td>Can anyone provide tips for clearing DNB CET MD/MS exam?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179464</th>\n",
       "      <td>e721130f8d3e47a54cad</td>\n",
       "      <td>What are some interesting AI use cases in ERP?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291567</th>\n",
       "      <td>391bdc2e29577623066d</td>\n",
       "      <td>Are there any social networking sites that are only for talking about politics?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84381</th>\n",
       "      <td>1085a12556be93f8e7c0</td>\n",
       "      <td>How do I earn money through online easily?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421290</th>\n",
       "      <td>5291ec9be39f9af6b763</td>\n",
       "      <td>Why is Thomas Paine not given more attention in our educational system?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599169</th>\n",
       "      <td>7559db840d7d214a84cf</td>\n",
       "      <td>Was Prophet Muhammad vegetarian?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384751</th>\n",
       "      <td>4b6071a64751870c0ef6</td>\n",
       "      <td>How does Facebook recognize videos that contain copyrighted materials and warns you after you posted them?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028969</th>\n",
       "      <td>c9a3081dd44460574b7f</td>\n",
       "      <td>What is Zamalek?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406440</th>\n",
       "      <td>4fa3b67539fcd5d25932</td>\n",
       "      <td>What is the process of posting of new PO in Public Sector banks? What are zones and circles in PSBs?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "244246   2fc50134cb76b8b254af   \n",
       "1114457  da61e1f4461d6a03ddfc   \n",
       "1179464  e721130f8d3e47a54cad   \n",
       "291567   391bdc2e29577623066d   \n",
       "84381    1085a12556be93f8e7c0   \n",
       "421290   5291ec9be39f9af6b763   \n",
       "599169   7559db840d7d214a84cf   \n",
       "384751   4b6071a64751870c0ef6   \n",
       "1028969  c9a3081dd44460574b7f   \n",
       "406440   4fa3b67539fcd5d25932   \n",
       "\n",
       "                                                                                                      question_text  \\\n",
       "244246   After bankruptcy do you sill have to pay liabilities?                                                        \n",
       "1114457  Can anyone provide tips for clearing DNB CET MD/MS exam?                                                     \n",
       "1179464  What are some interesting AI use cases in ERP?                                                               \n",
       "291567   Are there any social networking sites that are only for talking about politics?                              \n",
       "84381    How do I earn money through online easily?                                                                   \n",
       "421290   Why is Thomas Paine not given more attention in our educational system?                                      \n",
       "599169   Was Prophet Muhammad vegetarian?                                                                             \n",
       "384751   How does Facebook recognize videos that contain copyrighted materials and warns you after you posted them?   \n",
       "1028969  What is Zamalek?                                                                                             \n",
       "406440   What is the process of posting of new PO in Public Sector banks? What are zones and circles in PSBs?         \n",
       "\n",
       "         target  \n",
       "244246   0       \n",
       "1114457  0       \n",
       "1179464  0       \n",
       "291567   0       \n",
       "84381    0       \n",
       "421290   0       \n",
       "599169   0       \n",
       "384751   0       \n",
       "1028969  0       \n",
       "406440   0       "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sample sincere questions\")\n",
    "train_df.loc[train_df['target'] == 0].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    979943\n",
      "1    64954 \n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "target_count = train_df.target.value_counts(normalize=True)\n",
    "\n",
    "print(target_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_count.plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIn-67cmvDFk"
   },
   "source": [
    "## **Preparing the text data**\n",
    "\n",
    "First, we will iterate over the text questions are stored, and format them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bbmOu7FPvNgI",
    "outputId": "653bd496-0185-474a-d557-9d3d2dbacf08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1044897 training questions.\n",
      "Found 261225 validation questions.\n",
      "Found 56370 test questions.\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df['question_text'].tolist()\n",
    "X_val = val_df['question_text'].tolist()\n",
    "X_test = test_df['question_text'].tolist()\n",
    "\n",
    "print('Found %s training questions.' % len(X_train))\n",
    "print('Found %s validation questions.' % len(X_val))\n",
    "print('Found %s test questions.' % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_kbWqXCezaBA",
    "outputId": "79004f92-5f67-4ceb-caf0-01a78506cb9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1044897, 1000)\n",
      "Shape of y_train: (1044897,)\n",
      "Found 196192 unique tokens.\n",
      "CPU times: user 1min, sys: 3.24 s, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_WORDS = 50000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, lower=True, split=' ', \n",
    "                       char_level=False, oov_token=None, document_count=0,\n",
    "                      )\n",
    "                                   \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_train = train_df['target']\n",
    "y_val = val_df['target']\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "# Setup Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfAhNjEJEXmN",
    "outputId": "58618a30-4fab-44f0-8dec-591a8df2da47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195892 word vectors.\n",
      "CPU times: user 2min 35s, sys: 4.46 s, total: 2min 40s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('..', 'input', 'embeddings', 'glove.840B.300d', 'glove.840B.300d.txt'), buffering=((2<<16) + 8))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KeJJofUpQm9"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNd1KcNjsvxj"
   },
   "source": [
    "# Setup model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azpjSxsyyqOW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         58857900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 59,230,765\n",
      "Trainable params: 372,865\n",
      "Non-trainable params: 58,857,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "CPU times: user 732 ms, sys: 2.66 s, total: 3.39 s\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "x = embedding_layer(inp)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         58857900  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 128)         140544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                2048      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 16        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 59,001,088\n",
      "Trainable params: 142,898\n",
      "Non-trainable params: 58,858,190\n",
      "_________________________________________________________________\n",
      "None\n",
      "CPU times: user 7.64 s, sys: 228 ms, total: 7.87 s\n",
      "Wall time: 8.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from keras.layers import Dense, Dropout, Input, GlobalMaxPool1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Bidirectional\n",
    "from keras.layers import Activation, BatchNormalization, CuDNNGRU\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "x = embedding_layer(inp)\n",
    "x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "out = Activation(\"sigmoid\")(x)\n",
    "\n",
    "\n",
    "model = Model(inp, out)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lp4o2h9Fs8gP"
   },
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L75Rc0Gqtclm"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNd1KcNjsvxj"
   },
   "source": [
    "# Setup f1-score, precision and recall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.callbacks import Callback\\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, precision_recall_fscore_support\\n\\nclass Metrics(Callback):\\n\\n\\n    def on_train_begin(self, logs={}):\\n        self.val_f1s = []\\n        self.val_recalls = []\\n        self.val_precisions = []\\n \\n\\n    def on_epoch_end(self, epoch, logs={}):\\n        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\\n        val_targ = self.validation_data[1]\\n        _val_precision, _val_recall, _val_f1, _ = precision_recall_fscore_support(val_targ, val_predict, average=\\'binary\\')\\n        self.val_f1s.append(_val_f1)\\n        self.val_recalls.append(_val_recall)\\n        self.val_precisions.append(_val_precision)\\n        print(\" — val_f1: %f — val_precision: %f — val_recall: %f\" %(_val_f1, _val_precision, _val_recall))\\n        return\\n\\nmetrics = Metrics()\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, precision_recall_fscore_support\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    " \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_precision, _val_recall, _val_f1, _ = precision_recall_fscore_support(val_targ, val_predict, average='binary')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\" — val_f1: %f — val_precision: %f — val_recall: %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "metrics = Metrics()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Compute Class Weights\n",
    "\n",
    "Since there is a significant target inbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.utils import class_weight\\n\\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Train the Model\n",
    "\n",
    "Use model checkpointing to save the model that attains the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3OwX8ErsteZL",
    "outputId": "f718108c-5f78-4d9b-8814-c7044a9cce96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/2\n",
      "  75776/1044897 [=>............................] - ETA: 5:40 - loss: 0.6485 - acc: 0.7713"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "#          epochs=2, batch_size=128, callbacks=[metrics], class_weight=class_weights)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "          epochs=2, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Predict validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred_val = model.predict([X_val], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestThreshold(y_true,y_pred):\n",
    "    idx = 0\n",
    "    cur_f1 = 0\n",
    "    max_f1 = 0\n",
    "    thres = 0\n",
    "    for idx in np.arange(0.1, 0.501, 0.01):\n",
    "        cur_f1 = f1_score(y_true, np.array(y_pred)> idx)\n",
    "        if cur_f1 > max_f1:\n",
    "            max_f1 = cur_f1\n",
    "            thres = idx\n",
    "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(thres, max_f1))\n",
    "    return thres\n",
    "threshold = bestThreshold(y_val,pred_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Predict test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred_test = model.predict([X_test], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "submission_df['prediction'] = (pred_test > threshold).astype(int)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "capstone.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
