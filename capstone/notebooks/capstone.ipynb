{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85c12f788178232533cbf7f141e217c32b882cda",
    "colab_type": "text",
    "id": "w7oagxHHofxb"
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0421d98769ce1b3bcc0b65f729851be0d275cb5"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_WORDS = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "749cb0b7d343d12569698ece840039c7fef54711",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "## Common english contraction mappings (wikipedia):\n",
    "https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5a645a8f6a9f70d9db108cebbc683089e548260"
   },
   "outputs": [],
   "source": [
    "CONTRACTION_MAPPING = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56282f6e5433223f3823e24e95fe170534e8dcae"
   },
   "outputs": [],
   "source": [
    "PUNCT = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2a79c0d80f797d3868fac83217b1e2479bee8dc"
   },
   "outputs": [],
   "source": [
    "SPECIAL_PUNCT = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "451df1dd80192c375340a023e852cab39698d434"
   },
   "outputs": [],
   "source": [
    "PUNCT_MAPPING = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "228dc3ca14b0b0af33d6c874400d8f01ef96a874"
   },
   "outputs": [],
   "source": [
    "MISPELL_MAPPING = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "974ca8fb2157648f2ace9c41bf37e0d87dd2eb20",
    "colab_type": "text",
    "id": "w7oagxHHofxb"
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "408f2ecb4a7380c2a57635996ca248020e4aaa94",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "## Embedding helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88880adc1b1e2ca67e77db81d36165fad52d9e1c"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def loadEmbeddings(path, dimensions, mode='r', encoding=None, errors=None):\n",
    "    print('Loading embeddings from: %s' %path)\n",
    "    embeddings = {}\n",
    "    f = open(path, buffering=((2<<16) + 8), mode=mode, encoding=encoding, errors=errors)\n",
    "    for line in f:\n",
    "        if len(line) <= 100:\n",
    "            continue\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-dimensions])\n",
    "        coefs = np.asarray(values[-dimensions:], dtype='float32')\n",
    "        embeddings[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f202cf2766c8d2a73834f057bd195928136ce1b"
   },
   "outputs": [],
   "source": [
    "def loadEmbeddingsGensim(path, dimensions, binary=True):\n",
    "    print('Loading embeddings from: %s' %path)\n",
    "    embeddings = {}\n",
    "    gensim_vecs = KeyedVectors.load_word2vec_format(path, binary=binary)\n",
    "    for word, vector in zip(gensim_vecs.vocab, gensim_vecs.vectors):\n",
    "        coefs = np.asarray(vector[-dimensions:], dtype='float32')\n",
    "        embeddings[word] = coefs\n",
    "    print('Found %s word vectors.' % len(embeddings))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "344bdbebf5757946836407ee4a0a8ace39b65228",
    "colab": {},
    "colab_type": "code",
    "id": "0KeJJofUpQm9"
   },
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(embedding, word_index):\n",
    "    all_embs = np.stack(embedding.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    \n",
    "    nb_words = min(MAX_WORDS, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e462ed6a5cd6b481db42664c68c3451b61026d76",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "### Helper to replace contractions in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9cd4f331303cea868030ed771c30c01a65f74a7"
   },
   "outputs": [],
   "source": [
    "def clean_contractions(text):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([CONTRACTION_MAPPING[t] if t in CONTRACTION_MAPPING else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4d9fd63b54dda11ec562c8e2c85fa386e9910958",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "### Helper to remap punctuations in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1b038b3cadf6c09fcb2523a1171ad8e95a1700d"
   },
   "outputs": [],
   "source": [
    "def clean_special_chars(text):\n",
    "    for p in PUNCT_MAPPING:\n",
    "        text = text.replace(p, PUNCT_MAPPING[p])\n",
    "    \n",
    "    for p in PUNCT:\n",
    "        text = text.replace(p, ' ' + p + ' ')\n",
    "    \n",
    "    for s in SPECIAL_PUNCT:\n",
    "        text = text.replace(s, SPECIAL_PUNCT[s])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbcf585e91cd7faad3c230d6eb191dc39695db17",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "### Helper to correct common mispellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "186bf0c75630a56d02120c197712e4fb5681ad8d"
   },
   "outputs": [],
   "source": [
    "def correct_spelling(x):\n",
    "    for word in MISPELL_MAPPING.keys():\n",
    "        x = x.replace(word, MISPELL_MAPPING[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92cc1a4a5777c338f819adde4ec455d9856013dc",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "## Coverage helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20732a275083041d5a06ad4bd6a7938f7333afda"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            vocab[word] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b83aab607f791f5824b3eca4277741950cd36d92"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "70e187637082e5f448863c5f965fff8ab80ed065",
    "colab_type": "text",
    "id": "w7oagxHHofxb"
   },
   "source": [
    "# Import test/train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e20e43f9ea433ee2eee88003355ef86f724fa01c",
    "colab": {},
    "colab_type": "code",
    "id": "X3n8bRW6WcKo"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2052ad18f26ea6c28d4feeaac402d61438a6f1cf",
    "colab_type": "text",
    "id": "w7oagxHHofxb"
   },
   "source": [
    "# Analyze train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f55a111d1222a541a6cc5938de12577b3b12215",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "h0aRi6Toaq8j",
    "outputId": "5d5c6246-c8c3-49e0-e8b7-58850a83f62f"
   },
   "outputs": [],
   "source": [
    "print(\"Sample insincere questions\")\n",
    "train_df.loc[train_df['target'] == 1].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73bd9f42266c6c2184ff65017ccb5f62e38314f5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "n_mslOIdaj8G",
    "outputId": "c0c9da0b-96d8-436f-fe37-28a2da1d6b69"
   },
   "outputs": [],
   "source": [
    "print(\"Sample sincere questions\")\n",
    "train_df.loc[train_df['target'] == 0].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b8a948c3ae97a09decf8f0a6fadff9eed5f83b1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "target_ratios = train_df.target.value_counts(normalize=True)\n",
    "\n",
    "print(target_ratios)\n",
    "\n",
    "target_ratios.plot(kind='bar', title='Ratios (target)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "323d3c4ed532d94120a8681d822c5ad60fc6abae"
   },
   "outputs": [],
   "source": [
    "print('Average word length of questions in train is {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of questions in test is {0:.0f}.'.format(np.mean(test_df['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b49c4e9bbe8ab8ccddfc8b9c8b24bcafe1bbd3c"
   },
   "outputs": [],
   "source": [
    "print('Max word length of questions in train is {0:.0f}.'.format(np.max(train_df['question_text'].apply(lambda x: len(x.split())))))\n",
    "print('Max word length of questions in test is {0:.0f}.'.format(np.max(test_df['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "83ede400076de1e67b90de70553ae273d0b16473"
   },
   "outputs": [],
   "source": [
    "print('Average character length of questions in train is {0:.0f}.'.format(np.mean(train_df['question_text'].apply(lambda x: len(x)))))\n",
    "print('Average character length of questions in test is {0:.0f}.'.format(np.mean(test_df['question_text'].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b66752d7dfea96b9c5b171edfe6f6d2831956a1"
   },
   "outputs": [],
   "source": [
    "print('Max character length of questions in train is {0:.0f}.'.format(np.max(train_df['question_text'].apply(lambda x: len(x)))))\n",
    "print('Max character length of questions in test is {0:.0f}.'.format(np.max(test_df['question_text'].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ff56f79415f02f3a029a2a12c40444410427763"
   },
   "outputs": [],
   "source": [
    "print('p999 character length of questions in train is {0:.0f}.'.format(np.percentile(train_df['question_text'].apply(lambda x: len(x)), 99.9)))\n",
    "print('p999 character length of questions in test is {0:.0f}.'.format(np.percentile(test_df['question_text'].apply(lambda x: len(x)), 99.9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "755dc21b8d2ad16d4711e5f3cfc664617fba4a3f",
    "colab_type": "text",
    "id": "QIn-67cmvDFk"
   },
   "source": [
    "# Data processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e32a7c365a4f71149537c44eca72b762c0cf1a8b",
    "colab_type": "text",
    "id": "QIn-67cmvDFk"
   },
   "source": [
    "## TODO: Feature engineering\n",
    "\n",
    "Add feature engineering here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a65029f02944902ad1c4a3b988ba4d166ce26910"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_df['treated_question'] = train_df['question_text'].apply(lambda x: x.lower())\n",
    "train_df['treated_question'] = train_df['treated_question'].apply(lambda x: clean_contractions(x))\n",
    "train_df['treated_question'] = train_df['treated_question'].apply(lambda x: clean_special_chars(x))\n",
    "train_df['treated_question'] = train_df['treated_question'].apply(lambda x: correct_spelling(x))\n",
    "\n",
    "test_df['treated_question'] = test_df['question_text'].apply(lambda x: x.lower())\n",
    "test_df['treated_question'] = test_df['treated_question'].apply(lambda x: clean_contractions(x))\n",
    "test_df['treated_question'] = test_df['treated_question'].apply(lambda x: clean_special_chars(x))\n",
    "test_df['treated_question'] = test_df['treated_question'].apply(lambda x: correct_spelling(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21e24633c2528b71fc434bf528f7b29fbed63f25",
    "colab_type": "text",
    "id": "QIn-67cmvDFk"
   },
   "source": [
    "## Split train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a4e4292f236ac2a779968bb38239e61e5f48eab"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab9fc29433d90e5d99d5fe728bb92d3c613a1e9f",
    "colab_type": "text",
    "id": "QIn-67cmvDFk"
   },
   "source": [
    "## Fill data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3eff454f10f0f7ca9552205a993b094fa5b5c57",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bbmOu7FPvNgI",
    "outputId": "653bd496-0185-474a-d557-9d3d2dbacf08"
   },
   "outputs": [],
   "source": [
    "X_train = train_df['treated_question'].fillna('+++').tolist()\n",
    "X_val = val_df['treated_question'].fillna('+++').tolist()\n",
    "X_test = test_df['treated_question'].fillna('+++').tolist()\n",
    "\n",
    "y_train = train_df['target']\n",
    "y_val = val_df['target']\n",
    "\n",
    "print('Found %s training questions.' % len(X_train))\n",
    "print('Found %s validation questions.' % len(X_val))\n",
    "print('Found %s test questions.' % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9bb5e7c69abf941c142775505c87fb8804f6345",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_kbWqXCezaBA",
    "outputId": "79004f92-5f67-4ceb-caf0-01a78506cb9f"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, lower=True, split=' ', filters='',\n",
    "                       char_level=False, oov_token=None, document_count=0,\n",
    "                      )\n",
    "                                   \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aedafbcf978e9c8eb2db0fa3fc650762f02d87a0",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "## Save tokenized data + word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "test_df.to_pickle('test_df.pkl')\n",
    "\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_val.npy', X_val)\n",
    "np.save('X_test.npy', X_test)\n",
    "\n",
    "y_train.to_pickle('y_train.pkl')\n",
    "y_val.to_pickle('y_val.pkl')\n",
    "\n",
    "pickle.dump(word_index, open('word_index.pkl', 'wb'))\n",
    "\n",
    "del X_train\n",
    "del X_val\n",
    "del X_test\n",
    "del y_train\n",
    "del y_val\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aedafbcf978e9c8eb2db0fa3fc650762f02d87a0",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "## Build vocabulary with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "860e94814f74b4282fce01a2c8c42d7c29d1dabf"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_vocab = build_vocab(train_df['treated_question'])\n",
    "del train_df\n",
    "val_vocab = build_vocab(val_df['treated_question'])\n",
    "del val_df\n",
    "test_vocab = build_vocab(test_df['treated_question'])\n",
    "del test_df\n",
    "\n",
    "vocab = train_vocab + val_vocab + test_vocab\n",
    "del train_vocab\n",
    "del val_vocab\n",
    "del test_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1d40498bd469f8c5784570ddb88bb65a6367064",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "## Load embeddings, measure coverage and save embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNd1KcNjsvxj"
   },
   "source": [
    "# Restore point (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "import pickle\n",
    "\n",
    "word_index = pickle.load(open('word_index.pkl', 'rb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2801e906d9eb5bc96c231cf1e3d577a160d8d289",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfAhNjEJEXmN",
    "outputId": "58618a30-4fab-44f0-8dec-591a8df2da47"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import gc\n",
    "\n",
    "print('glove:')\n",
    "glove_path = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "embeddings_index = loadEmbeddings(glove_path, EMBEDDING_DIM)\n",
    "check_coverage(vocab, embeddings_index)\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index)\n",
    "del embeddings_index\n",
    "gc.collect()\n",
    "np.save('glove.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "889bc1ae4af9fe032c72e8c8c606ae5b1d66f7d8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfAhNjEJEXmN",
    "outputId": "58618a30-4fab-44f0-8dec-591a8df2da47"
   },
   "outputs": [],
   "source": [
    "print('paragram:')\n",
    "paragram_path = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "embeddings_index = loadEmbeddings(paragram_path, EMBEDDING_DIM, encoding='utf8', errors='ignore')\n",
    "check_coverage(vocab, embeddings_index)\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index)\n",
    "del embeddings_index\n",
    "gc.collect()\n",
    "np.save('paragram.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79b0a76992718f29aec733bec9f59beb85302a31",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YfAhNjEJEXmN",
    "outputId": "58618a30-4fab-44f0-8dec-591a8df2da47"
   },
   "outputs": [],
   "source": [
    "print('wiki:')\n",
    "wiki_path = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "embeddings_index = loadEmbeddings(wiki_path, EMBEDDING_DIM)\n",
    "check_coverage(vocab, embeddings_index)\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index)\n",
    "del embeddings_index\n",
    "gc.collect()\n",
    "np.save('wiki.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1ec208ac42fbf73e8213442f1333cbf18791a16"
   },
   "outputs": [],
   "source": [
    "print('google_news:')\n",
    "google_news_path = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = loadEmbeddingsGensim(google_news_path, EMBEDDING_DIM)\n",
    "check_coverage(vocab, embeddings_index)\n",
    "embedding_matrix = getEmbeddingMatrix(embeddings_index, word_index)\n",
    "del embeddings_index\n",
    "gc.collect()\n",
    "np.save('google_news.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7711de071093e6c256b2e8bf68fe64e9436ba0e2"
   },
   "outputs": [],
   "source": [
    "del word_index\n",
    "del vocab\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "380e34c9a8b4a797b10cb54e15760a9dd7b28ae0",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "# Oversample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47c26457a2b8c17015bf5c5c60cbb562fe199429"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42, ratio = 1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f28b53680bc8874337b09a61887d6c74da683659"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "balanced_train_df = pd.DataFrame()\n",
    "balanced_train_df[0] = np.array(y_train)\n",
    "\n",
    "target_ratios = balanced_train_df[0].value_counts(normalize=True)\n",
    "\n",
    "print(target_ratios)\n",
    "\n",
    "target_ratios.plot(kind='bar', title='Ratios after SMOTE (target)')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fab17b9cec7b6c69d9b96bfc5731882a1216d788",
    "colab_type": "text",
    "id": "aNd1KcNjsvxj"
   },
   "source": [
    "# Setup model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNd1KcNjsvxj"
   },
   "source": [
    "# Restore point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40 ms, sys: 1.12 s, total: 1.16 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "test_df = pd.read_pickle('test_df.pkl')\n",
    "\n",
    "X_train = np.load('X_train.npy')\n",
    "X_val = np.load('X_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "\n",
    "y_train = pd.read_pickle('y_train.pkl')\n",
    "y_val = pd.read_pickle('y_val.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d45a12e7532c1ff87d68b626baada8a5bbe6b0fe",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "## Embeddings Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "48576619ba4f1847df5ea12bdd11b24b3920b25d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layers = {}\n",
    "\n",
    "embedding_matrix = np.load('glove.npy')\n",
    "embedding_layers['glove'] = Embedding(MAX_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "embedding_matrix = np.load('paragram.npy')\n",
    "embedding_layers['paragram'] = Embedding(MAX_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "embedding_matrix = np.load('wiki.npy')\n",
    "embedding_layers['wiki'] = Embedding(MAX_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
    "\n",
    "embedding_matrix = np.load('google_news.npy')\n",
    "embedding_layers['google_news'] = Embedding(MAX_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "60a8b6ecb457fff127b4f7db954c42c63fa68b76",
    "colab_type": "text",
    "id": "BUSd8dmWEUf4"
   },
   "source": [
    "## NN: Bidirectional GRU per embedding with concactenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "2d9a7a2733cee240d25312c05c08d41e0d6c4976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 250, 300)     30000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 250, 300)     30000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 250, 300)     30000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 250, 300)     30000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 250, 128)     140544      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 250, 128)     140544      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 250, 128)     140544      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 250, 128)     140544      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 250, 128)     512         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 250, 128)     512         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 250, 128)     512         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 250, 128)     512         bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2048        global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           2048        global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2048        global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           2048        global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           64          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16)           64          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16)           64          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16)           64          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16)           0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16)           0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16)           0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16)           0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64)           0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2048        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32)           128         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32)           0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            32          activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 1)            4           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1)            0           batch_normalization_10[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 120,574,884\n",
      "Trainable params: 573,666\n",
      "Non-trainable params: 120,001,218\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "CPU times: user 8.4 s, sys: 2.96 s, total: 11.4 s\n",
      "Wall time: 9.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from keras.layers import Dense, Dropout, Input, GlobalMaxPool1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Bidirectional\n",
    "from keras.layers import Activation, BatchNormalization, CuDNNGRU\n",
    "from keras.layers import SpatialDropout1D, Concatenate, Flatten, Reshape\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "a = embedding_layers['glove'] (inp)\n",
    "a = Bidirectional(CuDNNGRU(64, return_sequences=True))(a)\n",
    "a = BatchNormalization()(a)\n",
    "a = GlobalMaxPool1D()(a)\n",
    "a = Dense(16, kernel_regularizer=l2(0.01), use_bias=False)(a)\n",
    "a = BatchNormalization()(a)\n",
    "a = Activation(\"relu\")(a)\n",
    "a = Dropout(0.1)(a)\n",
    "\n",
    "b = embedding_layers['paragram'] (inp)\n",
    "b = Bidirectional(CuDNNGRU(64, return_sequences=True))(b)\n",
    "b = BatchNormalization()(b)\n",
    "b = GlobalMaxPool1D()(b)\n",
    "b = Dense(16, kernel_regularizer=l2(0.01), use_bias=False)(b)\n",
    "b = BatchNormalization()(b)\n",
    "b = Activation(\"relu\")(b)\n",
    "b = Dropout(0.1)(b)\n",
    "\n",
    "c = embedding_layers['wiki'] (inp)\n",
    "c = Bidirectional(CuDNNGRU(64, return_sequences=True))(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = GlobalMaxPool1D()(c)\n",
    "c = Dense(16, kernel_regularizer=l2(0.01), use_bias=False)(c)\n",
    "c = BatchNormalization()(c)\n",
    "c = Activation(\"relu\")(c)\n",
    "c = Dropout(0.1)(c)\n",
    "\n",
    "d = embedding_layers['google_news'] (inp)\n",
    "d = Bidirectional(CuDNNGRU(64, return_sequences=True))(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = GlobalMaxPool1D()(d)\n",
    "d = Dense(16, kernel_regularizer=l2(0.01), use_bias=False)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = Activation(\"relu\")(d)\n",
    "d = Dropout(0.1)(d)\n",
    "\n",
    "x = Concatenate(axis=1)([a, b, c, d])\n",
    "x = Dense(32, kernel_regularizer=l2(0.01), use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Dense(1, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "out = Activation(\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74e7edd06d9d1ef89f4369021b8ce094be9e21db",
    "colab_type": "text",
    "id": "Lp4o2h9Fs8gP"
   },
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    " \n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2*((p*r)/(p+r+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "fafbbea8029e31d17a3f731639bbc62970e4e1b0",
    "colab": {},
    "colab_type": "code",
    "id": "L75Rc0Gqtclm"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', f1, recall, precision])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# serialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNd1KcNjsvxj"
   },
   "source": [
    "# Setup f1-score, precision and recall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "checkpoints = ModelCheckpoint('weights.h5', monitor=\"val_f1\", mode=\"max\", verbose=True, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.callbacks import Callback\\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, precision_recall_fscore_support\\n\\nclass Metrics(Callback):\\n\\n\\n    def on_train_begin(self, logs={}):\\n        self.val_f1s = []\\n        self.val_recalls = []\\n        self.val_precisions = []\\n \\n\\n    def on_epoch_end(self, epoch, logs={}):\\n        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\\n        val_targ = self.validation_data[1]\\n        _val_precision, _val_recall, _val_f1, _ = precision_recall_fscore_support(val_targ, val_predict, average=\\'binary\\')\\n        self.val_f1s.append(_val_f1)\\n        self.val_recalls.append(_val_recall)\\n        self.val_precisions.append(_val_precision)\\n        print(\" — val_f1: %f — val_precision: %f — val_recall: %f\" %(_val_f1, _val_precision, _val_recall))\\n        return\\n\\nmetrics = Metrics()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, precision_recall_fscore_support\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    " \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_precision, _val_recall, _val_f1, _ = precision_recall_fscore_support(val_targ, val_predict, average='binary')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(\" — val_f1: %f — val_precision: %f — val_recall: %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "metrics = Metrics()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Compute Class Weights\n",
    "\n",
    "Since there is a significant target inbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53314172 8.04336146]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "print (class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d1c4d092bca6f093254b5792c4b8801c1282b0f",
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "Use model checkpointing to save the model that attains the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2db478168d542638ce5cb6358de7ebd503d000b0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3OwX8ErsteZL",
    "outputId": "f718108c-5f78-4d9b-8814-c7044a9cce96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/8\n",
      "1044897/1044897 [==============================] - 529s 507us/step - loss: 0.4251 - acc: 0.9347 - f1: 0.5993 - recall: 0.7294 - precision: 0.5338 - val_loss: 0.2806 - val_acc: 0.9460 - val_f1: 0.6265 - val_recall: 0.7568 - val_precision: 0.5393\n",
      "Epoch 2/8\n",
      "1044897/1044897 [==============================] - 526s 503us/step - loss: 0.1671 - acc: 0.9583 - f1: 0.6442 - recall: 0.6172 - precision: 0.6855 - val_loss: 0.1338 - val_acc: 0.9591 - val_f1: 0.5915 - val_recall: 0.4983 - val_precision: 0.7434\n",
      "Epoch 3/8\n",
      "1044897/1044897 [==============================] - 524s 501us/step - loss: 0.1199 - acc: 0.9603 - f1: 0.6445 - recall: 0.5875 - precision: 0.7255 - val_loss: 0.1478 - val_acc: 0.9523 - val_f1: 0.6548 - val_recall: 0.7564 - val_precision: 0.5827\n",
      "Epoch 4/8\n",
      "1044897/1044897 [==============================] - 525s 503us/step - loss: 0.1039 - acc: 0.9620 - f1: 0.6588 - recall: 0.5992 - precision: 0.7449 - val_loss: 0.3370 - val_acc: 0.8792 - val_f1: 0.4835 - val_recall: 0.9393 - val_precision: 0.3276\n",
      "Epoch 5/8\n",
      "1044897/1044897 [==============================] - 524s 502us/step - loss: 0.0953 - acc: 0.9640 - f1: 0.6809 - recall: 0.6264 - precision: 0.7580 - val_loss: 0.5443 - val_acc: 0.7854 - val_f1: 0.3538 - val_recall: 0.9735 - val_precision: 0.2172\n",
      "Epoch 6/8\n",
      " 969728/1044897 [==========================>...] - ETA: 34s - loss: 0.0885 - acc: 0.9668 - f1: 0.7096 - recall: 0.6620 - precision: 0.7760"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#hist = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "#          epochs=2, batch_size=128, callbacks=[metrics], class_weight=class_weights)\n",
    "\n",
    "#hist = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "#          epochs=2, batch_size=128, class_weight=class_weights)\n",
    "\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_val, y_val), callbacks=[checkpoints], epochs=8, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "926710a7b8364a65a6ea5bdb8f103462b622549c",
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Predict validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80dbd1aae9b5ec74071b7a11de96abee7cd2ad66"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred_val = model.predict([X_val], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fbe1a9626a625a4540b35fe646af0d78e8952b24",
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Find optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5861eb15765c20d4a5be5892543520bdb006bfa"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def optimalThreshold(y_true,y_pred):\n",
    "    idx = 0\n",
    "    cur_f1 = 0\n",
    "    cur_prec = 0\n",
    "    cur_recall = 0\n",
    "    max_f1 = 0\n",
    "    thres = 0\n",
    "    for idx in np.arange(0.1, 0.501, 0.01):\n",
    "        cur_f1 = f1_score(y_true, np.array(y_pred)> idx)\n",
    "        cur_recall = recall_score(y_true, np.array(y_pred)> idx)\n",
    "        cur_prec = precision_score(y_true, np.array(y_pred)> idx)\n",
    "        print('Current threshold is {:.4f} with F1 score: {:.4f}, Recall score: {:.4f}, Precision score: {:.4f}'\n",
    "              .format(idx, cur_f1, cur_recall, cur_prec)\n",
    "             )\n",
    "        if cur_f1 > max_f1:\n",
    "            max_f1 = cur_f1\n",
    "            thres = idx\n",
    "    print('optimal threshold is {:.4f} with F1 score: {:.4f}'.format(thres, max_f1))\n",
    "    return thres\n",
    "threshold = optimalThreshold(y_val,pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_val, np.array(pred_val > threshold).astype(int), target_names=['sincere', 'insincere']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cm = confusion_matrix(y_val, np.array(pred_val > threshold).astype(int))\n",
    "ax = plt.subplot()\n",
    "hm = sns.heatmap(cm, annot=True, ax = ax, fmt='g', \n",
    "                 cmap=ListedColormap(['white']), linecolor='black', \n",
    "                 linewidth=1, cbar=False,\n",
    "                 xticklabels = 1, yticklabels = 1 )\n",
    "\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels') \n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['sincere', 'insincere'])\n",
    "ax.yaxis.set_ticklabels(['sincere', 'insincere'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ff585d4c11ae673051881795e74ff97d8d2b4c8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title('model accuracy')\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ff585d4c11ae673051881795e74ff97d8d2b4c8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title('model loss')\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ff585d4c11ae673051881795e74ff97d8d2b4c8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title('model F1-score')\n",
    "plt.plot(hist.history['f1'])\n",
    "plt.plot(hist.history['val_f1'])\n",
    "plt.ylabel('f1 score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# serialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights.h5\")\n",
    "print(\"Saved model weights to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d377f47a86a67e1b60bf83efa2e85c08e87281dd",
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Predict test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0670f1a7c1ed5cfd85d049dfc0ef1681aaab055b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred_test = model.predict([X_test], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3c1ff0ac4324bbf847ed3fdafe4db682d833f8a",
    "colab_type": "text",
    "id": "NVFKqCPUtJcy"
   },
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f3601299112b7d1ea815db80243344ace06ca42"
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "submission_df['prediction'] = (pred_test > threshold).astype(int)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "capstone.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
